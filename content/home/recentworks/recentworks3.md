+++
title = "Engenharia de Dados na Transição à Plataforma SAP"
type = "recentwork"
weight = 2
date = "2015-06-24"
description= "Adequação/Enriquecimento/Unificação das Bases de Dados de Clientes/Especialistas para dar Suporte na Transição à Plataforma SAP."
image= "/post/recentworks/Engenharia2.jpg"
+++





A imagem que abre este post busca ilustrar a complexa parte artística, principalmente no quesito criatividade, do trabalho que envolve a engenharia de dados. Há tantos caminhos pelos quais o mesmo trabalho poderia ser realizado, os quais os objetivos primordiais estariam ainda bem retratados. O resultado final é produto de um improviso constante que, embasado em um propósito, se mantém fiel as necessidades da demanda. Como a própria palavra criatividade indica, a medida que o fluxo de dados vai sendo gerado, percebe-se o poder que existe no ato de criar quando em atividade, não se trata de planejar toda uma solução, com um resultado final também bem estabelecido para o "processo de desenvolvimento". Aqui trata-se de, enquanto em atividade, buscando construir um fluxo de dados que transforme de um estado bruto para um estado mais adequado às análises que serão desenvolvidas posteriormente, ou mesmo incluindo tais análises dentro do fluxo quando coerente, enquanto em atividade, durante esta construção, vá se percebendo como as alterações, que estão em constante desenvolvimento, estão cada vez mais relacionadas com o real propósito de negócio que dá origem ao projeto, encontrando uma solução cada vez mais adequada a medida que se exercita esta atividade de busca através do ato de fazer, realmente colocando a mão na massa e permitindo que sua capacidade criativa compreenda como cada passo-a-passo executado no fluxo se associa com o próposito final estabelecido para assim permitir compreender os possíveis outros passos, na sequência de desenvolvimento, que também serviriam aos objetivos estabelecidos, levando em consideração os empecilhos e dificuldades que serão encontrados ao longo do fluxo em construção, devido a decisões inadequadas que possam naturalmente ter ocorrido e que possuem fundamental importância ao longo de todo este processo.  

Esta imagem também busca transmitir esta mesma ideia a partir da reflexão sobre um tipo diferente de quebra-cabeça. Um quebra-cabeça o qual suas peças quando todas bem unidas não irão mostrar uma solução ideal, pois há inúmeras maneiras de construir um determinado propósito a partir de distintas estratégias e formas de se conceber um fluxo de dados específico, mas ainda assim, com todas estas peças unidas fica explícito que, na imagem acima, trata-se de um leão. A solução do problema passa pela sua quebra em problemas menores, pequenas peças que, buscando fazer parte do quebra-cabeça maior, ainda precisam ser testadas quanto a seus pares, seus encaixes. O elemento teste, na construção deste quebra-cabeça, é algo fundamental, realmente de importância significativa. Por mais que a lógica que tenha sido desenvolvida lhe soe bastante razoável e perspicaz na construção de determinado passo, dentro do fluxo, é preciso estabelecer mecanismos credíveis de se avaliar se as transformações ocasionadas por esta lógica se refletiram conforme o esperado na base de dados, ou seja, por exemplo, se o focinho do leão quando acrescentado as demais peças a sua volta, também ajuda na compreensão das feições, em essência, que deseja-se expressar na imagem, ou seja, se ainda vamos continuar conseguindo enxergar o leão. 

A ideia deste quebra-cabeça também é diferente no seguinte sentido: as peças deste quebra-cabeça não necessariamente pertencem exclusivamente a ele. Algumas destas peças poderiam ser utilizadas para compor um quebra-cabeça distinto, um outro animal por exemplo. Perceba que os olhos desta imagem, é uma peça que poderia ser reutilizada para compor uma outra figura, uma certa espécie de lobo, um lince, uma onça. Estas peças são as denominadas MACROS e representam construções de fluxos que podem ser encapsulados como uma ferramenta única, visto a sua função que pode ser aproveitada em demais outros problemas, servindo assim, como uma peça também adequada para se contruir uma solução para problemas distintos. 

O engenheiro de dados encontra-se constantemente nesta complexa função ao desenvolver diferentes fluxos de dados, seja para integrar diferentes fontes de dados em um data lake, seja para construir bases mais refinadas para alimentar um data warehouse, para subsidiar determinadas operações em um data mart específico, para adequar a qualidade dos dados, para gerar uma base analítica, para otimizar um fluxo já existente, para produtizar e tornar acessível determinadas análises a um maior número de pessoas, etc. 

O papel de engenharia dentro deste cargo reside na ideia primária da existência de um sistema sobre o qual a função do engenheiro se mantêm comprometida, recebendo matérias-primas, processando estas matérias-primas e agregando camadas de valor que serão consolidadas em um produto final. A diferença entre engenharias se manifesta exatamente nos inputs que serão recebidos para processamento e nas técnicas/recursos e conhecimentos disponíveis para serem empregados neste processamento, que irá transformar as matérias-primas do sistema, agregando valor ao material bruto e resultando em um produto final de maior utilidade. Como exemplo, dentro de uma das minhas áreas de formação, temos a Engenharia de Produção sendo definida como:

> A engenharia de produção trata do projeto, aperfeiçoamento e implantação de sistemas integrados de pessoas, materiais, informações, equipamentos e energia, para a produção de bens e serviços, de maneira econômica, respeitando os preceitos éticos e culturais. Tem como base os conhecimentos específicos e as habilidades associadas às ciências físicas, matemáticas e sociais, assim como aos princípios e métodos de análise da engenharia de projeto, para especificar, predizer e avaliar os resultados obtidos por tais sistemas.
>
>-- <cite>Mário Otávio Batalha - Introdução à Engenharia de Produção (Prêmio Jabuti 2008).</cite>

As definições das distintas engenharias são muito aproximadas entre elas e, utilizando como base a descrição acima, podemos perceber que para a Engenharia de Dados, nós também buscamos produzir bens e/ou serviços, com a mentalidade em minimizar os custos e/ou maximizar a utilidade, também é nosso papel projetar, aperfeiçoar e/ou implantar sistemas, utilizando dos conhecimentos, próprios ou relativos a área, para especificar, predizer e avaliar os resultados obtidos pelos sistemas desenvolvidos, e, sem sombra de dúvida, talvez com ainda mais atenção nesta área da engenharia, existe a necessidade de alicerçar todo este sistema nos preceitos éticos e culturais existentes. 

A real diferença reside nos inputs que iremos utilizar dentro do nosso sistema, onde a fonte primordial com a qual nós iremos trabalhar, embora também seja de importância significativa as pessoas, informações, equipamentos e energia, mesmo que em diferentes graus quando comparados a engenharia de produção, a grande diferença encontra-se no material primário de trabalho, que prenuncia a existência da área, e que, para o Engenheiro de Dados, são os próprios dados, podendo estar nos mais diferentes graus de refinamento e originados das mais diversas fontes possíveis. 

Por fim, a segunda diferença significativa é o arcabouço técnico utilizado para processar a nossa matéria prima, que embora pertençam sim às grandes áreas das ciências físicas, matemáticas e sociais, representam nichos específicos dentro dessas grandes áreas, ainda assim existindo interseções, como por exemplo em sistemas de informação e pesquisa operacional, grandes áreas da engenharia de produção, assim como possíveis semelhanças quanto aos princípios da engenharia de projeto, contidas no uso das metodologias ágeis, sendo estas de importância inequívoca para manter a eficiência e a organização do trabalho do engenheiro de dados. 

No entanto, embora apresente certas interseções e semelhanças, e assim será, independente da engenharia que utilizemos para comparar, perceba que o conjunto de técnicas e teorias utilizadas se difere em muito, sendo mais apropriado buscar estes recursos na área de ciência de dados e analytics.

Para o seguinte trabalho, alguns termos e os conceitos que os definem são importantes para o bom entendimento das necessidades da demanda:

- O que significa adequação das bases? Na terminologia utilizada dentro do projeto.
- Como podemos definir o processo de enriquecimento?
- Como podemos definir o processo de unificação?

Desta forma, o seguinte post busca descrever um projeto realizado para o grupo Fleury S.A., empresa referência na área de saúde, listada na bolsa de valores brasileira (FLRY3), tendo quase 100 anos de funcionamento, sendo fundada em 1926. O foco do projeto é criar fluxos de dados que permitam dar suporte nos processos que antecedem a implementação efetiva do novo ERP contratado para a empresa, requisitos principalmente voltados para verificações e tratamentos na qualidade dos dados, demandados pela plataforma SAP. 

Estes processos, no caso específico, envolvem mecanismos eficientes de validação dos campos contidos nas principais bases que irão alimentar o sistema; Aplicação destes tratamentos em bases terceiras que servirão para adicionar novos campos as bases já existentes, complementando a base original para padronizar dados existentes entre as várias entidades (clientes/especialistas) que compõem a base, seguindo a lógica de um processo de enriquecimento; Indicação de registros apropriados para se adicionar/atualizar em produção e registros inconsistentes que necessitam de análise manual e correção antes de serem repassado a base em produção, seguindo a lógica de um processo de unificação.



## Contextualização do Projeto

Projeto, no formato de automatização, tendo como produto final dois tipos de fluxos:

1. Fluxos de dados responsáveis por aplicar os tratamentos necessários e atualizar as principais bases de operação da empresa, assim como indicar registros inconsistentes que necessitem de verificação manual.

2. Fluxos de dados, encapsulados em ferramenta única (denominadas macros) para cada campo a ser avaliado, responsáveis pela validação dos principais campos existentes nas bases operacionais, permitindo reprodução e fácil utilização em qualquer outro processo de preparação de dados necessário.

Os fluxos de dados entregues ao final do projeto, foram construídos utilizando bases internas da empresa e bases terceiras contratadas para ajudar no processo de enriquecimento. Desta forma, visto as demandas iniciais de ajuste na qualidade dos dados que seriam semelhantes nos dois processos necessários a finalização do projeto (Enriquecimento e Unificação), assim como do entendimento dos dados e semelhança de campos entre bases operacionais (Clientes e Especialistas), torna-se apropriado a construção de macros que dêem a liberdade para o cliente reproduzir estes fluxos, já criados e encapsulados, em demais operações e análises que sejam necessárias dentro dos vários departamentos da empresa.

Este projeto, embora inserido em um contexto específico de "tratamentos necessários previamente à transição de um ERP", pertence a um contexto maior que envolve a geração de uma "pipeline" de dados, função característica do engenheiro de dados, compondo uma de suas principais atribuições. Esta "pipeline" é sub-componente de uma "pipeline" maior, aquela que vai da fonte de geração dos dados (a alimentação do sistema com dados de novos clientes e especialistas) até as mãos dos potenciais analistas que se servirão destes valores, sendo usuários do banco de dados.

Lembrando que uma "pipeline" de dados, nada mais é do que um conjunto de etapas que visa levar os dados de um ponto A para um ponto B, adequando estes dados para aqueles que serão os usuários no ponto B.

A exceção para o processo de ETL encontra-se na adição das dimensões de processamento, sequenciamento e dependências, monitoramento, assim como, a definição dos storages, fatores estes com peculiaridades intrínsecas a cada demanda.

Na nossa "pipeline" específica, dentro do contexto da demanda, temos as seguintes descrições para as componentes da "pipeline":

- Origem: Fontes de dados internas contemplando Clientes e Especialistas; Fontes de dados terceiras complementando informações das bases internas.

- Destino: Neste projeto o destino permanece o mesmo da origem. Os fluxos terão o objetivo de modificar a base original, apurando a qualidade dos dados, enriquecendo a base original com informações complementares e retornando a base processada para sua origem, tomando o lugar da atual base de dados em produção.

- Principais Transformações: Deduplicações, padronizações, atualização/preenchimento por correspondência (Lógica Fuzzy) e validações.  

- Sequenciamento e Dependências: Macros de validação (Base de Enriquecimento) -> Enriquecimento -> Macros de Validação (Base Principal) -> Unificação -> Macros de Validação -> Monitoramento.

- Monitoramento: Alertas gerados pelos registros inconsistentes em acordo com a etapa de detecção da inconsistência (i.g. Enriquecimento, Unificação, Final). E-mail enviado aos responsáveis pelas verificações manuais com as identificações dos registros inconsistentes e o diagnóstico superficial. 

### Projeto



Projeto contratado com urgência pelo grupo Fleury, para atuar na estratégia de mudança de ERP da empresa, adquirindo plataforma SAP e necessitando verificar e executar, dentro das regras exigidas pela nova plataforma, todo o processo de checagem da qualidade dos dados, adequando as bases aos processos de validação exigidos pela plataforma.

O projeto, além do objetivo principal mencionado em parágrafo anterior, também tinha como principais atividades, realizar, através de bases terceiras contratadas pelo grupo Fleury, todo o processo de enriquecimento das bases de dados de clientes e profissionais de saúde, para, em seguida, trabalhar no processo de unificação, destas bases já enriquecidas, nos bancos de dados principais da empresa.

Desta forma, para avaliar a qualidade dos dados e adequar aos processos exigidos pela plataforma SAP, foi convertido, todas as regras de negócio definidas em documento redigido pelo grupo Fleury, em diferentes fluxos de dados, transformando cada um destes fluxos em macros que permitissem automatizar os processos futuros, sendo os fatores de repetitividade e reprodutibilidade essenciais para o sucesso do projeto, facilitando o entendimento da lógica utilizada para conversão das regras de negócio, através de documentação passo-a-passo nos fluxos desenvolvidos, e assim permitindo também reproduzir a mesma lógica em ferramentas distintas. Ao todo, foram desenvolvidos 10 macros para verificar os campos de nome completo, RG, CPF, CRM, especialidade, telefone, e-mail, data de nascimento e óbito, passaporte e sexo.

Como exemplo de validação, tomemos o caso de CPF's inconsistentes. Qual seria o procedimento formal para avaliar se um dado CPF apresenta numeração válida ou não?

Neste exemplo, o fluxo criado avalia a estrutura e os dígitos verificadores do CPF, considerando os dados como consistentes quando as seguintes regras são válidas:

```
1. Recusar todos os dígitos iguais.
2. Realizar o cálculo do dígito verificador conforme padrão do CPF.
3. Possui onze caracteres.
4. Aceitar o preenchimento apenas com números.
5. Campos em branco ou nulos são considerados inconsistentes.

```

A imagem abaixo é uma representação de um fluxo desenvolvido para considerar as 5 regras descritas acima:

![LeonardoDantas](/post/recentworks/Macro_CPF.PNG)

Assim, para implementação destas regras, faz-se uso de expressões regulares como por exemplo na regra 1 e regra 4. As expressões regulares permitem avaliar padrões de caracteres para operacionalizar a partir da correspondência entre o padrão e o campo analisado. 

Na regra 1, por exemplo, utilizamos o seguinte RegEx (Expressão Regular) para avaliar se há ou não correspondência do campo avaliando com o padrão definido no RegEx, resultando em uma nova coluna com apenas 2 possíveis valores, Verdadeiro ou Falso:

```yaml
RegEx: "^(.)\1{8}$"

Parâmetros:

  ^ & $: "Marcadores utilizados para definir o início e o fim do padrão de texto, delimitando o CPF para conter apenas o que fora especificado na expressão."

  (.): 'O ponto significa "qualquer caractere (exceto quebras de linha)" e os parênteses formam um grupo de captura para utilizar posteriormente como referência. Isso quer dizer que o primeiro caractere do CPF será capturado por este grupo.'

  \1: 'Representa uma backreference, ou seja, ela significa "o mesmo texto que foi capturado no primeiro grupo", ou seja, para o nosso caso, o primeiro caractere do CPF.'

  {8}: "Qualifica a referência indicada pelo backreference, definindo a quantidade de repetições que se deve buscar do grupo de captura referenciado."

```

Desta forma, caso o campo analisado possua um valor de CPF cujo os 8 últimos números sejam iguais ao primeiro número, o RegEx acima apontará como "Verdadeiro" para o campo analisado, sendo necessário posteriormente filtrar todos estes valores verdadeiros julgados como inconsistentes.

Dentre as regras descritas acima, outra de maior curiosidade é o cálculo do dígito verificador conforme padrão do CPF. Uma regra que lembro que fiquei surpreso ao realizar o trabalho pois não sabia da existência de um raciocínio matemático por trás da estrutura do CPF.

```
Validação do primeiro dígito verificador:

No exemplo, vamos supor que o valor de CPF seja 029.278.583-65.

Devemos então multiplicar os 9 primeiros dígitos do CPF pela sequência decrescente de números de 10 à 2, posteriormente somando os resultados. 

0 * 10 + 2 * 9 + 9 * 8 + 2 * 7 + 7 * 6 + 8 * 5 + 5 * 4 + 8 * 3 + 3 * 2 = 236

O próximo passo da verificação é multiplicar o resultado do somatório por 10 e dividir por 11.

(236 * 10) / 11 = 214 com RESTO da divisão igual a 6.

O primeiro dígito verificador (primeiro dígito depois do '-') deve ser equivalente ao RESTO desta divisão. Caso apresente divergências o CPF já pode ser julgado como inconsistente.

```
O segundo dígito verificador apresenta regras semelhantes:

```
Validação do segundo dígito verificador:

Utilizando o mesmo CPF da primeira verificação, devemos agora multiplicar os 10 primeiros dígitos (não apenas os 9 primeiros) do CPF pela sequência decrescente de números de 11 à 2, posteriormente somando os resultados. 

0 * 11 + 2 * 10 + 9 * 9 + 2 * 8 + 7 * 7 + 8 * 6 + 5 * 5 + 8 * 4 + 3 * 3 + 6 * 2 = 292

O próximo passo da verificação é multiplicar o resultado do somatório por 10 e dividir por 11.

(292 * 10) / 11 = 265 com RESTO da divisão igual a 5.

Assim, como o RESTO desta divisão corresponde ao segundo dígito verificador e sabendo que o primeiro dígito já fora validado, podemos considerar este CPF como consistente quanto a estas regras.  

```
Para avaliar demais regras possíveis de se utilizar na validação destes e demais campos, recomendo o documento aberto da SIPREV (Sistema integrado de informações previdenciárias), redigindo detalhes das regras de negócio utilizadas em projeto próprio. Segue o link: http://sa.previdencia.gov.br/site/2015/07/rgrv_RegrasValidacao.pdf.

Desta forma, baseado nos exemplos acima, foram construídos fluxos de dados que continham as várias regras implementadas neles e como dito estes fluxos foram encapsulados em ferramenta única como ilustrado no gif abaixo:

![LeonardoDantas](/post/recentworks/Macro_CPF2.gif)

No gif acima, é possível perceber que a nova ferramenta, com ícone caracterizando o CPF, quando aberta e avaliada sua composição, nada mais é do que o mesmo fluxo desenvolvido para considerar as 5 regras descritas acima, equivalente a imagem já mostrada anteriormente neste post.

Dando continuidade ao projeto, para melhor adequar a qualidade dos dados às exigências na nova plataforma, foi utilizado lógica fuzzy para tratar os registros de clientes e especialistas imprecisamente preenchidos, permitindo preencher conforme determinado padrão, registros que, embora apresentem preenchimentos distintos, ainda se tratam da mesma pessoa.

Os processos de enriquecimento e unificação já se beneficiaram das macros anteriormente desenvolvidas, sendo utilizadas para avaliar a consistência dos campos, gerando revalidações das bases terceiras e permitindo a correta associação entre as bases terceiras e bases próprias.

No fim, foi gerado um fluxo para cada processo, enriquecimento e unificação, juntamente com as macros de validação das regras de negócio, sendo possível realizar os processos de unificação e enriquecimento dos dados já validados, assim como enviar para a equipe responsável os diferentes registros que apresentavam algum tipo de inconsistência, identificando a razão da inconsistência e podendo então liberar tempo para a equipe focar nas causas e soluções para as inconsistências encontradas, antes de seguir com estes registros para as bases principais.



## Leonardo Dantas

Consultor em análise de dados com 4+ anos de experiência trabalhando em projetos para grandes empresas de diferentes segmentos do mercado como Fleury S.A., Souza Cruz Ltda., KPMG Consultoria Ltda., etc. Mente orientada para possibilidade de produtização dos serviços ao longo do período de desenvolvimento. Desde estagiário atuando em toda cadeia de atividades de consultoria, a começar por relacionamento com os clientes, apresentando portfólio de serviços e acompanhando os serviços já prestados, até as etapas de apresentação e fechamento do projeto (gestão do conhecimento), passando por atividades de entendimento de demandas/coleta de requisitos, atividades de desenvolvimento, como provas de conceito/estudos/projetos, e atividades de monitoramento e acompanhamento dos projetos em execução. Especialista na construção de fluxos ETL e no manuseio do software Alteryx Designer, ministrando cursos e treinamentos nesta ferramenta para grandes consultorias como Falconi Consultores S.A. e KPMG Consultoria Ltda. Movido por desafios e sempre interessado em buscar formas mais eficientes de se resolver problemas, acreditando fortemente que o potencial humano de tomar decisões coerentes pode ser significativamente alavancado com o uso de tecnologias da área de ciência de dados e de metodologias da área de analytics.




